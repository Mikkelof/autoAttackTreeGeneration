CWE-ID,Name,Weakness Abstraction,Status,Description,Extended Description,Related Weaknesses,Weakness Ordinalities,Applicable Platforms,Background Details,Alternate Terms,Modes Of Introduction,Exploitation Factors,Likelihood of Exploit,Common Consequences,Detection Methods,Potential Mitigations,Observed Examples,Functional Areas,Affected Resources,Taxonomy Mappings,Related Attack Patterns,Notes
1427,Improper Neutralization of Input Used for LLM Prompting,Base,Incomplete,"The product uses externally-provided data to build prompts provided to large language models (LLMs), but the way these prompts are constructed causes the LLM to fail to distinguish between user-supplied inputs and developer provided system directives.","When prompts are constructed using externally controllable data, it is often possible to cause an LLM to ignore the original guidance provided by its creators (known as the system prompt) by inserting malicious instructions in plain human language or using bypasses such as special characters or tags. Because LLMs are designed to treat all instructions as legitimate, there is often no way for the model to differentiate between what prompt language is malicious when it performs inference and returns data. Many LLM systems incorporate data from other adjacent products or external data sources like Wikipedia using API calls and retrieval augmented generation (RAG). Any external sources in use that may contain untrusted data should also be considered potentially malicious.",::NATURE:ChildOf:CWE ID:77:VIEW ID:1000:ORDINAL:Primary::,,::LANGUAGE CLASS:Not Language-Specific:LANGUAGE PREVALENCE:Undetermined::OPERATING SYSTEM CLASS:Not OS-Specific:OPERATING SYSTEM PREVALENCE:Undetermined::ARCHITECTURE CLASS:Not Architecture-Specific:ARCHITECTURE PREVALENCE:Undetermined::TECHNOLOGY NAME:AI/ML:TECHNOLOGY PREVALENCE:Undetermined::,,"::TERM:prompt injection:DESCRIPTION:attack-oriented term for modifying prompts, whether due to this weakness or other weaknesses::","::PHASE:Architecture and Design:NOTE:LLM-connected applications that do not distinguish between trusted and untrusted input may introduce this weakness. If such systems are designed in a way where trusted and untrusted instructions are provided to the model for inference without differentiation, they may be susceptible to prompt injection and similar attacks.::PHASE:Implementation:NOTE:When designing the application, input validation should be applied to user input used to construct LLM system prompts. Input validation should focus on mitigating well-known software security risks (in the event the LLM is given agency to use tools or perform API calls) as well as preventing LLM-specific syntax from being included (such as markup tags or similar).::PHASE:Implementation:NOTE:This weakness could be introduced if training does not account for potentially malicious inputs.::PHASE:System Configuration:NOTE:Configuration could enable model parameters to be manipulated when this was not intended.::PHASE:Integration:NOTE:This weakness can occur when integrating the model into the software.::PHASE:Bundling:NOTE:This weakness can occur when bundling the model with the software.::",,,"::SCOPE:Confidentiality:SCOPE:Integrity:SCOPE:Availability:IMPACT:Execute Unauthorized Code or Commands:IMPACT:Varies by Context:NOTE:The consequences are entirely contextual, depending on the system that the model is integrated into. For example, the consequence could include output that would not have been desired by the model designer, such as using racial slurs. On the other hand, if the output is attached to a code interpreter, remote code execution (RCE) could result.::SCOPE:Confidentiality:IMPACT:Read Application Data:NOTE:An attacker might be able to extract sensitive information from the model.::SCOPE:Integrity:IMPACT:Modify Application Data:IMPACT:Execute Unauthorized Code or Commands:NOTE:The extent to which integrity can be impacted is dependent on the LLM application use case.::SCOPE:Access Control:IMPACT:Read Application Data:IMPACT:Modify Application Data:IMPACT:Gain Privileges or Assume Identity:NOTE:The extent to which access control can be impacted is dependent on the LLM application use case.::","::METHOD:Dynamic Analysis with Manual Results Interpretation:DESCRIPTION:Use known techniques for prompt injection and other attacks, and adjust the attacks to be more specific to the model or system.::METHOD:Dynamic Analysis with Automated Results Interpretation:DESCRIPTION:Use known techniques for prompt injection and other attacks, and adjust the attacks to be more specific to the model or system.::METHOD:Architecture or Design Review:DESCRIPTION:Review of the product design can be effective, but it works best in conjunction with dynamic analysis.::","::PHASE:Architecture and Design:DESCRIPTION:LLM-enabled applications should be designed to ensure proper sanitization of user-controllable input, ensuring that no intentionally misleading or dangerous characters can be included. Additionally, they should be designed in a way that ensures that user-controllable input is identified as untrusted and potentially dangerous.:EFFECTIVENESS:High::PHASE:Implementation:DESCRIPTION:LLM prompts should be constructed in a way that effectively differentiates between user-supplied input and developer-constructed system prompting to reduce the chance of model confusion at inference-time.:EFFECTIVENESS:Moderate::PHASE:Architecture and Design:DESCRIPTION:LLM-enabled applications should be designed to ensure proper sanitization of user-controllable input, ensuring that no intentionally misleading or dangerous characters can be included. Additionally, they should be designed in a way that ensures that user-controllable input is identified as untrusted and potentially dangerous.:EFFECTIVENESS:High::PHASE:Implementation:DESCRIPTION:Ensure that model training includes training examples that avoid leaking secrets and disregard malicious inputs. Train the model to recognize secrets, and label training data appropriately. Note that due to the non-deterministic nature of prompting LLMs, it is necessary to perform testing of the same test case several times in order to ensure that troublesome behavior is not possible. Additionally, testing should be performed each time a new model is used or a model's weights are updated.::PHASE:Installation Operation:DESCRIPTION:During deployment/operation, use components that operate externally to the system to monitor the output and act as a moderator. These components are called different terms, such as supervisors or guardrails.::PHASE:System Configuration:DESCRIPTION:During system configuration, the model could be fine-tuned to better control and neutralize potentially dangerous inputs.::","::REFERENCE:CVE-2023-32786:DESCRIPTION:Chain: LLM integration framework has prompt injection (CWE-1427) that allows an attacker to force the service to retrieve data from an arbitrary URL, essentially providing SSRF (CWE-918) and potentially injecting content into downstream tasks.:LINK:https://www.cve.org/CVERecord?id=CVE-2023-32786::REFERENCE:CVE-2024-5184:DESCRIPTION:ML-based email analysis product uses an API service that allows a malicious user to inject a direct prompt and take over the service logic, forcing it to leak the standard hard-coded system prompts and/or execute unwanted prompts to leak sensitive data.:LINK:https://www.cve.org/CVERecord?id=CVE-2024-5184::REFERENCE:CVE-2024-5565:DESCRIPTION:Chain: library for generating SQL via LLMs using RAG uses a prompt function to present the user with visualized results, allowing altering of the prompt using prompt injection (CWE-1427) to run arbitrary Python code (CWE-94) instead of the intended visualization code.:LINK:https://www.cve.org/CVERecord?id=CVE-2024-5565::",,,,,,
